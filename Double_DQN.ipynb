{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\n",
    "\n",
    "https://github.com/yfeng997/MadMario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchrl\\data\\replay_buffers\\samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import imageio\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, time, datetime, os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cuda\")))\n",
    "        self.batch_size = 512\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini CNN structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 41 - Epsilon 0.9999897500512483 - Mean Reward 232.0 - Mean Length 41.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.197 - Time 2025-02-25T18:27:24\n",
      "Episode 10 - Step 4034 - Epsilon 0.9989920082391394 - Mean Reward 551.182 - Mean Length 366.727 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 17.966 - Time 2025-02-25T18:27:42\n",
      "Episode 20 - Step 7682 - Epsilon 0.9980813427402506 - Mean Reward 568.762 - Mean Length 365.81 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.337 - Time 2025-02-25T18:27:58\n",
      "Episode 30 - Step 10104 - Epsilon 0.997477187338122 - Mean Reward 592.323 - Mean Length 325.935 - Mean Loss 0.139 - Mean Q Value 0.024 - Time Delta 11.845 - Time 2025-02-25T18:28:10\n",
      "Episode 40 - Step 12150 - Epsilon 0.9969671081568396 - Mean Reward 583.488 - Mean Length 296.341 - Mean Loss 0.444 - Mean Q Value 0.407 - Time Delta 22.849 - Time 2025-02-25T18:28:33\n",
      "Episode 50 - Step 17450 - Epsilon 0.9956470013372366 - Mean Reward 595.941 - Mean Length 342.157 - Mean Loss 0.515 - Mean Q Value 0.698 - Time Delta 61.16 - Time 2025-02-25T18:29:34\n",
      "Episode 60 - Step 20077 - Epsilon 0.9949933247618475 - Mean Reward 555.033 - Mean Length 329.131 - Mean Loss 0.558 - Mean Q Value 0.906 - Time Delta 30.238 - Time 2025-02-25T18:30:04\n",
      "Episode 70 - Step 22578 - Epsilon 0.9943713995573407 - Mean Reward 566.056 - Mean Length 318.0 - Mean Loss 0.595 - Mean Q Value 1.276 - Time Delta 29.307 - Time 2025-02-25T18:30:33\n",
      "Episode 80 - Step 25208 - Epsilon 0.9937178151697541 - Mean Reward 565.901 - Mean Length 311.21 - Mean Loss 0.616 - Mean Q Value 1.57 - Time Delta 33.184 - Time 2025-02-25T18:31:06\n",
      "Episode 90 - Step 27631 - Epsilon 0.9931160528050884 - Mean Reward 562.473 - Mean Length 303.637 - Mean Loss 0.63 - Mean Q Value 1.803 - Time Delta 29.939 - Time 2025-02-25T18:31:36\n",
      "Episode 100 - Step 29527 - Epsilon 0.9926454272842262 - Mean Reward 556.82 - Mean Length 294.86 - Mean Loss 0.645 - Mean Q Value 2.018 - Time Delta 23.285 - Time 2025-02-25T18:32:00\n",
      "Episode 110 - Step 33677 - Epsilon 0.9916160915848322 - Mean Reward 590.7 - Mean Length 296.43 - Mean Loss 0.741 - Mean Q Value 2.58 - Time Delta 49.723 - Time 2025-02-25T18:32:49\n",
      "Episode 120 - Step 36305 - Epsilon 0.9909648136993672 - Mean Reward 590.17 - Mean Length 286.23 - Mean Loss 0.831 - Mean Q Value 3.187 - Time Delta 31.875 - Time 2025-02-25T18:33:21\n",
      "Episode 130 - Step 38635 - Epsilon 0.9903877447108327 - Mean Reward 583.24 - Mean Length 285.31 - Mean Loss 0.876 - Mean Q Value 3.798 - Time Delta 28.024 - Time 2025-02-25T18:33:49\n",
      "Episode 140 - Step 42104 - Epsilon 0.9895292031706808 - Mean Reward 584.32 - Mean Length 299.54 - Mean Loss 0.836 - Mean Q Value 4.372 - Time Delta 41.463 - Time 2025-02-25T18:34:31\n",
      "Episode 150 - Step 45617 - Epsilon 0.9886605305509872 - Mean Reward 581.37 - Mean Length 281.67 - Mean Loss 0.858 - Mean Q Value 5.003 - Time Delta 42.874 - Time 2025-02-25T18:35:14\n",
      "Episode 160 - Step 48861 - Epsilon 0.9878590518039754 - Mean Reward 606.32 - Mean Length 287.84 - Mean Loss 0.88 - Mean Q Value 5.628 - Time Delta 40.033 - Time 2025-02-25T18:35:54\n",
      "Episode 170 - Step 51860 - Epsilon 0.987118681968062 - Mean Reward 589.01 - Mean Length 292.82 - Mean Loss 0.918 - Mean Q Value 6.266 - Time Delta 37.357 - Time 2025-02-25T18:36:31\n",
      "Episode 180 - Step 54782 - Epsilon 0.9863978549947857 - Mean Reward 581.91 - Mean Length 295.74 - Mean Loss 0.959 - Mean Q Value 6.934 - Time Delta 35.044 - Time 2025-02-25T18:37:06\n",
      "Episode 190 - Step 59079 - Epsilon 0.9853387859208492 - Mean Reward 601.95 - Mean Length 314.48 - Mean Loss 0.996 - Mean Q Value 7.586 - Time Delta 49.394 - Time 2025-02-25T18:37:55\n",
      "Episode 200 - Step 64610 - Epsilon 0.9839772500930091 - Mean Reward 632.61 - Mean Length 350.83 - Mean Loss 1.052 - Mean Q Value 8.396 - Time Delta 54.752 - Time 2025-02-25T18:38:50\n",
      "Episode 210 - Step 68224 - Epsilon 0.9830886280318144 - Mean Reward 608.84 - Mean Length 345.47 - Mean Loss 1.083 - Mean Q Value 9.072 - Time Delta 36.754 - Time 2025-02-25T18:39:27\n",
      "Episode 220 - Step 71255 - Epsilon 0.9823439746968995 - Mean Reward 626.43 - Mean Length 349.5 - Mean Loss 1.127 - Mean Q Value 9.791 - Time Delta 31.99 - Time 2025-02-25T18:39:59\n",
      "Episode 230 - Step 73361 - Epsilon 0.9818269066596238 - Mean Reward 620.19 - Mean Length 347.26 - Mean Loss 1.184 - Mean Q Value 10.647 - Time Delta 22.156 - Time 2025-02-25T18:40:21\n",
      "Episode 240 - Step 75487 - Epsilon 0.9813052042481458 - Mean Reward 609.23 - Mean Length 333.83 - Mean Loss 1.227 - Mean Q Value 11.395 - Time Delta 22.616 - Time 2025-02-25T18:40:44\n",
      "Episode 250 - Step 79194 - Epsilon 0.9803962008108836 - Mean Reward 605.79 - Mean Length 335.77 - Mean Loss 1.261 - Mean Q Value 12.044 - Time Delta 39.442 - Time 2025-02-25T18:41:23\n",
      "Episode 260 - Step 83727 - Epsilon 0.9792857959793326 - Mean Reward 619.8 - Mean Length 348.66 - Mean Loss 1.316 - Mean Q Value 12.854 - Time Delta 48.586 - Time 2025-02-25T18:42:12\n",
      "Episode 270 - Step 86410 - Epsilon 0.9786291601933559 - Mean Reward 632.83 - Mean Length 345.5 - Mean Loss 1.345 - Mean Q Value 13.497 - Time Delta 28.965 - Time 2025-02-25T18:42:41\n",
      "Episode 280 - Step 91543 - Epsilon 0.977374139591302 - Mean Reward 663.24 - Mean Length 367.61 - Mean Loss 1.38 - Mean Q Value 14.13 - Time Delta 55.016 - Time 2025-02-25T18:43:36\n",
      "Episode 290 - Step 95996 - Epsilon 0.9762866831120921 - Mean Reward 652.94 - Mean Length 369.17 - Mean Loss 1.426 - Mean Q Value 14.882 - Time Delta 47.986 - Time 2025-02-25T18:44:24\n",
      "Episode 300 - Step 99886 - Epsilon 0.9753377057090635 - Mean Reward 630.94 - Mean Length 352.76 - Mean Loss 1.451 - Mean Q Value 15.455 - Time Delta 41.38 - Time 2025-02-25T18:45:05\n",
      "Episode 310 - Step 102564 - Epsilon 0.9746849355725924 - Mean Reward 625.91 - Mean Length 343.4 - Mean Loss 1.502 - Mean Q Value 16.146 - Time Delta 27.072 - Time 2025-02-25T18:45:32\n",
      "Episode 320 - Step 105171 - Epsilon 0.9740498915538118 - Mean Reward 591.97 - Mean Length 339.16 - Mean Loss 1.537 - Mean Q Value 16.762 - Time Delta 25.964 - Time 2025-02-25T18:45:58\n",
      "Episode 330 - Step 108330 - Epsilon 0.9732809392357459 - Mean Reward 596.45 - Mean Length 349.69 - Mean Loss 1.56 - Mean Q Value 17.239 - Time Delta 33.051 - Time 2025-02-25T18:46:31\n",
      "Episode 340 - Step 115311 - Epsilon 0.9715838018594459 - Mean Reward 615.91 - Mean Length 398.24 - Mean Loss 1.593 - Mean Q Value 17.783 - Time Delta 69.999 - Time 2025-02-25T18:47:41\n",
      "Episode 350 - Step 117227 - Epsilon 0.9711185246029297 - Mean Reward 616.66 - Mean Length 380.33 - Mean Loss 1.628 - Mean Q Value 18.389 - Time Delta 18.978 - Time 2025-02-25T18:48:00\n",
      "Episode 360 - Step 122058 - Epsilon 0.9699463640390998 - Mean Reward 592.99 - Mean Length 383.31 - Mean Loss 1.648 - Mean Q Value 18.849 - Time Delta 49.389 - Time 2025-02-25T18:48:50\n",
      "Episode 370 - Step 125666 - Epsilon 0.9690718667668585 - Mean Reward 590.65 - Mean Length 392.56 - Mean Loss 1.671 - Mean Q Value 19.333 - Time Delta 35.661 - Time 2025-02-25T18:49:25\n",
      "Episode 380 - Step 132927 - Epsilon 0.9673143544854621 - Mean Reward 586.3 - Mean Length 413.84 - Mean Loss 1.688 - Mean Q Value 19.776 - Time Delta 72.917 - Time 2025-02-25T18:50:38\n",
      "Episode 390 - Step 137750 - Epsilon 0.9661487179310292 - Mean Reward 580.41 - Mean Length 417.54 - Mean Loss 1.705 - Mean Q Value 20.174 - Time Delta 47.552 - Time 2025-02-25T18:51:26\n",
      "Episode 400 - Step 143129 - Epsilon 0.9648503624575329 - Mean Reward 588.31 - Mean Length 432.43 - Mean Loss 1.725 - Mean Q Value 20.584 - Time Delta 56.168 - Time 2025-02-25T18:52:22\n",
      "Episode 410 - Step 145274 - Epsilon 0.9643331000893027 - Mean Reward 582.1 - Mean Length 427.1 - Mean Loss 1.722 - Mean Q Value 20.889 - Time Delta 22.529 - Time 2025-02-25T18:52:44\n",
      "Episode 420 - Step 148576 - Epsilon 0.9635373714977664 - Mean Reward 589.57 - Mean Length 434.05 - Mean Loss 1.725 - Mean Q Value 21.214 - Time Delta 34.882 - Time 2025-02-25T18:53:19\n",
      "Episode 430 - Step 152109 - Epsilon 0.9626867027398304 - Mean Reward 590.72 - Mean Length 437.79 - Mean Loss 1.734 - Mean Q Value 21.541 - Time Delta 37.341 - Time 2025-02-25T18:53:57\n",
      "Episode 440 - Step 156565 - Epsilon 0.9616148667423449 - Mean Reward 598.87 - Mean Length 412.54 - Mean Loss 1.734 - Mean Q Value 21.825 - Time Delta 47.231 - Time 2025-02-25T18:54:44\n",
      "Episode 450 - Step 159082 - Epsilon 0.961009960850226 - Mean Reward 596.14 - Mean Length 418.55 - Mean Loss 1.738 - Mean Q Value 22.068 - Time Delta 26.591 - Time 2025-02-25T18:55:10\n",
      "Episode 460 - Step 161261 - Epsilon 0.9604865931737733 - Mean Reward 605.68 - Mean Length 392.03 - Mean Loss 1.756 - Mean Q Value 22.409 - Time Delta 21.115 - Time 2025-02-25T18:55:32\n",
      "Episode 470 - Step 166015 - Epsilon 0.959345732805583 - Mean Reward 622.22 - Mean Length 403.49 - Mean Loss 1.774 - Mean Q Value 22.769 - Time Delta 45.84 - Time 2025-02-25T18:56:17\n",
      "Episode 480 - Step 172617 - Epsilon 0.957763638458116 - Mean Reward 614.6 - Mean Length 396.9 - Mean Loss 1.789 - Mean Q Value 23.137 - Time Delta 63.828 - Time 2025-02-25T18:57:21\n",
      "Episode 490 - Step 177287 - Epsilon 0.956646101759355 - Mean Reward 597.95 - Mean Length 395.37 - Mean Loss 1.795 - Mean Q Value 23.455 - Time Delta 45.435 - Time 2025-02-25T18:58:07\n",
      "Episode 500 - Step 182066 - Epsilon 0.9555038311860948 - Mean Reward 611.31 - Mean Length 389.37 - Mean Loss 1.796 - Mean Q Value 23.747 - Time Delta 46.22 - Time 2025-02-25T18:58:53\n",
      "Episode 510 - Step 185195 - Epsilon 0.9547566804880676 - Mean Reward 622.62 - Mean Length 399.21 - Mean Loss 1.802 - Mean Q Value 24.026 - Time Delta 30.226 - Time 2025-02-25T18:59:23\n",
      "Episode 520 - Step 187970 - Epsilon 0.9540945476620005 - Mean Reward 624.49 - Mean Length 393.94 - Mean Loss 1.808 - Mean Q Value 24.289 - Time Delta 28.274 - Time 2025-02-25T18:59:51\n",
      "Episode 530 - Step 192459 - Epsilon 0.9530244155121896 - Mean Reward 643.07 - Mean Length 403.5 - Mean Loss 1.813 - Mean Q Value 24.596 - Time Delta 43.143 - Time 2025-02-25T19:00:34\n",
      "Episode 540 - Step 195510 - Epsilon 0.9522977732067631 - Mean Reward 650.37 - Mean Length 389.45 - Mean Loss 1.824 - Mean Q Value 24.96 - Time Delta 29.541 - Time 2025-02-25T19:01:04\n",
      "Episode 550 - Step 198554 - Epsilon 0.951573350188621 - Mean Reward 659.24 - Mean Length 394.72 - Mean Loss 1.827 - Mean Q Value 25.337 - Time Delta 29.502 - Time 2025-02-25T19:01:34\n",
      "Episode 560 - Step 204920 - Epsilon 0.9500601254803898 - Mean Reward 672.31 - Mean Length 436.59 - Mean Loss 1.819 - Mean Q Value 25.665 - Time Delta 61.837 - Time 2025-02-25T19:02:35\n",
      "Episode 570 - Step 211102 - Epsilon 0.94859294143137 - Mean Reward 669.13 - Mean Length 450.87 - Mean Loss 1.809 - Mean Q Value 25.905 - Time Delta 59.834 - Time 2025-02-25T19:03:35\n",
      "Episode 580 - Step 215770 - Epsilon 0.9474865790182947 - Mean Reward 672.44 - Mean Length 431.53 - Mean Loss 1.81 - Mean Q Value 26.214 - Time Delta 45.322 - Time 2025-02-25T19:04:21\n",
      "Episode 590 - Step 221097 - Epsilon 0.9462256034473665 - Mean Reward 704.86 - Mean Length 438.1 - Mean Loss 1.814 - Mean Q Value 26.514 - Time Delta 53.408 - Time 2025-02-25T19:05:14\n",
      "Episode 600 - Step 224418 - Epsilon 0.9454403255754218 - Mean Reward 686.46 - Mean Length 423.52 - Mean Loss 1.827 - Mean Q Value 26.951 - Time Delta 32.03 - Time 2025-02-25T19:05:46\n",
      "Episode 610 - Step 227075 - Epsilon 0.9448125252918426 - Mean Reward 680.6 - Mean Length 418.8 - Mean Loss 1.828 - Mean Q Value 27.392 - Time Delta 25.758 - Time 2025-02-25T19:06:12\n",
      "Episode 620 - Step 232019 - Epsilon 0.9436454582605392 - Mean Reward 717.76 - Mean Length 440.49 - Mean Loss 1.841 - Mean Q Value 27.859 - Time Delta 47.665 - Time 2025-02-25T19:06:59\n",
      "Episode 630 - Step 235268 - Epsilon 0.9428792933420423 - Mean Reward 689.06 - Mean Length 428.09 - Mean Loss 1.849 - Mean Q Value 28.366 - Time Delta 31.97 - Time 2025-02-25T19:07:31\n",
      "Episode 640 - Step 237953 - Epsilon 0.9422465979093742 - Mean Reward 661.38 - Mean Length 424.43 - Mean Loss 1.853 - Mean Q Value 28.856 - Time Delta 27.552 - Time 2025-02-25T19:07:59\n",
      "Episode 650 - Step 242539 - Epsilon 0.9411669310870358 - Mean Reward 664.34 - Mean Length 439.85 - Mean Loss 1.874 - Mean Q Value 29.398 - Time Delta 46.749 - Time 2025-02-25T19:08:46\n",
      "Episode 660 - Step 246463 - Epsilon 0.9402440989353361 - Mean Reward 652.02 - Mean Length 415.43 - Mean Loss 1.884 - Mean Q Value 29.915 - Time Delta 41.251 - Time 2025-02-25T19:09:27\n",
      "Episode 670 - Step 252196 - Epsilon 0.9388974591782574 - Mean Reward 643.16 - Mean Length 410.94 - Mean Loss 1.9 - Mean Q Value 30.487 - Time Delta 59.078 - Time 2025-02-25T19:10:26\n",
      "Episode 680 - Step 258546 - Epsilon 0.9374081417339375 - Mean Reward 630.46 - Mean Length 427.76 - Mean Loss 1.897 - Mean Q Value 30.954 - Time Delta 63.207 - Time 2025-02-25T19:11:29\n",
      "Episode 690 - Step 262660 - Epsilon 0.936444512970615 - Mean Reward 602.85 - Mean Length 415.63 - Mean Loss 1.892 - Mean Q Value 31.383 - Time Delta 44.043 - Time 2025-02-25T19:12:13\n",
      "Episode 700 - Step 268423 - Epsilon 0.9350963018206546 - Mean Reward 625.47 - Mean Length 440.05 - Mean Loss 1.877 - Mean Q Value 31.655 - Time Delta 58.93 - Time 2025-02-25T19:13:12\n",
      "Episode 710 - Step 272481 - Epsilon 0.9341481275465279 - Mean Reward 631.16 - Mean Length 454.06 - Mean Loss 1.881 - Mean Q Value 31.998 - Time Delta 40.393 - Time 2025-02-25T19:13:53\n",
      "Episode 720 - Step 274685 - Epsilon 0.9336335536419846 - Mean Reward 587.73 - Mean Length 426.66 - Mean Loss 1.881 - Mean Q Value 32.34 - Time Delta 23.397 - Time 2025-02-25T19:14:16\n",
      "Episode 730 - Step 280840 - Epsilon 0.9321980295712559 - Mean Reward 626.01 - Mean Length 455.72 - Mean Loss 1.884 - Mean Q Value 32.66 - Time Delta 62.296 - Time 2025-02-25T19:15:18\n",
      "Episode 740 - Step 284416 - Epsilon 0.931365016840748 - Mean Reward 649.99 - Mean Length 464.63 - Mean Loss 1.886 - Mean Q Value 32.979 - Time Delta 38.245 - Time 2025-02-25T19:15:57\n",
      "Episode 750 - Step 286912 - Epsilon 0.9307840262854412 - Mean Reward 624.71 - Mean Length 443.73 - Mean Loss 1.873 - Mean Q Value 33.192 - Time Delta 25.81 - Time 2025-02-25T19:16:22\n",
      "Episode 760 - Step 288354 - Epsilon 0.930448539077185 - Mean Reward 615.0 - Mean Length 418.91 - Mean Loss 1.861 - Mean Q Value 33.369 - Time Delta 14.524 - Time 2025-02-25T19:16:37\n",
      "Episode 770 - Step 291337 - Epsilon 0.9297549156593462 - Mean Reward 609.87 - Mean Length 391.41 - Mean Loss 1.866 - Mean Q Value 33.637 - Time Delta 30.55 - Time 2025-02-25T19:17:07\n",
      "Episode 780 - Step 294723 - Epsilon 0.9289682110444929 - Mean Reward 621.02 - Mean Length 361.77 - Mean Loss 1.881 - Mean Q Value 33.961 - Time Delta 34.743 - Time 2025-02-25T19:17:42\n",
      "Episode 790 - Step 298999 - Epsilon 0.9279756745079798 - Mean Reward 630.72 - Mean Length 363.39 - Mean Loss 1.884 - Mean Q Value 34.245 - Time Delta 42.241 - Time 2025-02-25T19:18:24\n",
      "Episode 800 - Step 300041 - Epsilon 0.9277339682981234 - Mean Reward 599.9 - Mean Length 316.18 - Mean Loss 1.894 - Mean Q Value 34.589 - Time Delta 10.257 - Time 2025-02-25T19:18:35\n",
      "Episode 810 - Step 303870 - Epsilon 0.9268463197644494 - Mean Reward 613.35 - Mean Length 313.89 - Mean Loss 1.918 - Mean Q Value 35.044 - Time Delta 38.795 - Time 2025-02-25T19:19:13\n",
      "Episode 820 - Step 306192 - Epsilon 0.9263084415427759 - Mean Reward 617.89 - Mean Length 315.07 - Mean Loss 1.924 - Mean Q Value 35.479 - Time Delta 23.379 - Time 2025-02-25T19:19:37\n",
      "Episode 830 - Step 309981 - Epsilon 0.925431411210838 - Mean Reward 604.32 - Mean Length 291.41 - Mean Loss 1.921 - Mean Q Value 35.885 - Time Delta 37.096 - Time 2025-02-25T19:20:14\n",
      "Episode 840 - Step 314187 - Epsilon 0.9244588313836957 - Mean Reward 601.45 - Mean Length 297.71 - Mean Loss 1.942 - Mean Q Value 36.335 - Time Delta 40.922 - Time 2025-02-25T19:20:55\n",
      "Episode 850 - Step 316797 - Epsilon 0.9238558186750461 - Mean Reward 632.76 - Mean Length 298.85 - Mean Loss 1.955 - Mean Q Value 36.88 - Time Delta 25.48 - Time 2025-02-25T19:21:20\n",
      "Episode 860 - Step 320839 - Epsilon 0.9229227337740565 - Mean Reward 650.61 - Mean Length 324.85 - Mean Loss 1.985 - Mean Q Value 37.47 - Time Delta 38.727 - Time 2025-02-25T19:21:59\n",
      "Episode 870 - Step 324156 - Epsilon 0.922157717240159 - Mean Reward 650.37 - Mean Length 328.19 - Mean Loss 1.995 - Mean Q Value 38.021 - Time Delta 31.908 - Time 2025-02-25T19:22:31\n",
      "Episode 880 - Step 327170 - Epsilon 0.9214631330307649 - Mean Reward 646.79 - Mean Length 324.47 - Mean Loss 1.991 - Mean Q Value 38.505 - Time Delta 29.224 - Time 2025-02-25T19:23:00\n",
      "Episode 890 - Step 330743 - Epsilon 0.9206404034905153 - Mean Reward 671.06 - Mean Length 317.44 - Mean Loss 2.009 - Mean Q Value 39.094 - Time Delta 34.452 - Time 2025-02-25T19:23:35\n",
      "Episode 900 - Step 335552 - Epsilon 0.9195342285096656 - Mean Reward 683.94 - Mean Length 355.11 - Mean Loss 2.044 - Mean Q Value 39.721 - Time Delta 46.535 - Time 2025-02-25T19:24:21\n",
      "Episode 910 - Step 339789 - Epsilon 0.9185607274378222 - Mean Reward 669.06 - Mean Length 359.19 - Mean Loss 2.025 - Mean Q Value 40.06 - Time Delta 41.25 - Time 2025-02-25T19:25:02\n",
      "Episode 920 - Step 341135 - Epsilon 0.9182516837138838 - Mean Reward 666.87 - Mean Length 349.43 - Mean Loss 2.048 - Mean Q Value 40.428 - Time Delta 13.3 - Time 2025-02-25T19:25:16\n",
      "Episode 930 - Step 344947 - Epsilon 0.9173770065996921 - Mean Reward 653.54 - Mean Length 349.66 - Mean Loss 2.062 - Mean Q Value 40.775 - Time Delta 38.972 - Time 2025-02-25T19:25:55\n",
      "Episode 940 - Step 349097 - Epsilon 0.9164257214008252 - Mean Reward 646.57 - Mean Length 349.1 - Mean Loss 2.039 - Mean Q Value 41.023 - Time Delta 40.361 - Time 2025-02-25T19:26:35\n",
      "Episode 950 - Step 352015 - Epsilon 0.9157574325414324 - Mean Reward 631.76 - Mean Length 352.18 - Mean Loss 2.06 - Mean Q Value 41.341 - Time Delta 30.188 - Time 2025-02-25T19:27:05\n",
      "Episode 960 - Step 357766 - Epsilon 0.9144417481673964 - Mean Reward 628.89 - Mean Length 369.27 - Mean Loss 2.054 - Mean Q Value 41.673 - Time Delta 61.149 - Time 2025-02-25T19:28:06\n",
      "Episode 970 - Step 360725 - Epsilon 0.9137655399429254 - Mean Reward 632.36 - Mean Length 365.69 - Mean Loss 2.056 - Mean Q Value 41.96 - Time Delta 32.416 - Time 2025-02-25T19:28:39\n",
      "Episode 980 - Step 364078 - Epsilon 0.9129998968282605 - Mean Reward 633.06 - Mean Length 369.08 - Mean Loss 2.09 - Mean Q Value 42.42 - Time Delta 36.452 - Time 2025-02-25T19:29:15\n",
      "Episode 990 - Step 366861 - Epsilon 0.9123648979964116 - Mean Reward 610.69 - Mean Length 361.18 - Mean Loss 2.101 - Mean Q Value 42.917 - Time Delta 29.897 - Time 2025-02-25T19:29:45\n",
      "Episode 999 - Step 370019 - Epsilon 0.9116448700877583 - Mean Reward 627.96 - Mean Length 349.47 - Mean Loss 2.09 - Mean Q Value 43.326 - Time Delta 34.195 - Time 2025-02-25T19:30:19\n",
      "Final model saved as mario_final.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 1000\n",
    "for e in range(episodes):\n",
    "\n",
    "    frames = []\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 10 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n",
    "        \n",
    "# Save the final model at the end\n",
    "torch.save(mario.net.state_dict(), \"mario_final.pth\")\n",
    "print(\"Final model saved as mario_final.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zly20\\AppData\\Local\\Temp\\ipykernel_10144\\1079015317.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mario.net.load_state_dict(torch.load(\"mario_final.pth\"))\n",
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\zly20\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames captured: 168\n",
      "🎞️ Gameplay saved as mario_play.gif 🎮\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment (compatible with Gym v0.26+)\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "\n",
    "# Limit action space\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "# Load trained model\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=len(SIMPLE_MOVEMENT), save_dir=save_dir)\n",
    "mario.net.load_state_dict(torch.load(\"mario_final.pth\"))\n",
    "mario.net.eval()  # Set to evaluation mode\n",
    "\n",
    "# List to store frames\n",
    "frames = []\n",
    "\n",
    "# Reset environment & get initial state\n",
    "state, _ = env.reset()  # ✅ FIX: Ensure only the state is used\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # Convert state correctly for Mario's act() method\n",
    "    state_np = state.__array__()  # ✅ Ensure it's a NumPy array\n",
    "\n",
    "    # Get action from trained model\n",
    "    with torch.no_grad():\n",
    "        action = mario.act(state_np)  # ✅ Pass correct state format\n",
    "\n",
    "    # Perform action in the environment\n",
    "    next_state, reward, done, trunc, _ = env.step(action)\n",
    "    state = next_state  # Update state for the next iteration\n",
    "\n",
    "    # Capture frame for GIF\n",
    "    frame = env.render()\n",
    "    if isinstance(frame, np.ndarray):  # ✅ Ensure it's a NumPy array before converting\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "    time.sleep(0.02)  # Small delay for smoother playback\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Debug: Check number of frames\n",
    "print(f\"Total frames captured: {len(frames)}\")\n",
    "if len(frames) <= 1:\n",
    "    print(\"⚠️ Warning: Only 1 frame captured! Check rendering or loop conditions.\")\n",
    "\n",
    "# Save gameplay as a GIF\n",
    "imageio.mimsave(\"mario_play.gif\", frames, fps=30)\n",
    "print(\"🎞️ Gameplay saved as mario_play.gif 🎮\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
